{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccdaafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "MPS available: False\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "Sat Jan  3 13:13:39 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0             50W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "# On Colab with GPU this prints GPU info; locally you'll see MPS True and CUDA False\n",
    "!nvidia-smi  # works only on machines with Nvidia GPUs (Colab/GPU instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AV1D20-45wzi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AV1D20-45wzi",
    "outputId": "db0b041d-ddf6-4f1c-d811-85ccd21db8ea"
   },
   "outputs": [],
   "source": "# Step 1: Install compatible versions\n!pip install -q transformers==4.36.0 accelerate==0.25.0 datasets==2.16.0 evaluate sentencepiece rouge-score\n\n# Step 2: MUST RESTART RUNTIME NOW!\n# Click: Runtime -> Restart runtime\n# Then continue from the next cell\n\nprint(\"âœ… Installation complete!\")\nprint(\"âš ï¸  NOW RESTART THE RUNTIME BEFORE CONTINUING!\")\nprint(\"   Click: Runtime -> Restart runtime\")"
  },
  {
   "cell_type": "code",
   "id": "ph14dm6sk68",
   "source": "# OPTION 2: Run this to automatically restart (recommended!)\nimport os\nos.kill(os.getpid(), 9)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xstcxjfsj2a",
   "source": "## ðŸš€ Quick Setup Guide\n\n**Follow these 3 steps to fix the import error:**\n\n### Step 1: Install packages\nRun the **installation cell above** (the one with `pip install`)\n\n### Step 2: Restart runtime (CHOOSE ONE)\n- **OPTION A (Easy)**: Run the cell with `os.kill(os.getpid(), 9)` - auto-restarts!\n- **OPTION B (Manual)**: Click `Runtime` â†’ `Restart runtime`\n\n### Step 3: Verify\nAfter restart, run the **verification cell** to confirm everything works\n\nThen continue with the rest of the notebook!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6kstn9luy2j",
   "source": "## ðŸ”„ RESTART REQUIRED!\n\n**After running the installation cell above:**\n\n1. Click `Runtime` menu at the top\n2. Select `Restart runtime`  \n3. Come back and run the verification cell below\n\n**DO NOT skip the restart** - it's required to clear cached imports!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w9u928q21v",
   "source": "# âœ… VERIFICATION CELL - Run this after restarting runtime\n\ntry:\n    from transformers import T5ForConditionalGeneration, T5TokenizerFast, Seq2SeqTrainer, Seq2SeqTrainingArguments\n    import torch\n    import evaluate\n    import transformers\n    import accelerate\n    \n    print(\"=\" * 60)\n    print(\"âœ… SUCCESS! All imports working correctly!\")\n    print(\"=\" * 60)\n    print(f\"âœ“ Transformers: {transformers.__version__}\")\n    print(f\"âœ“ Accelerate: {accelerate.__version__}\")\n    print(f\"âœ“ PyTorch: {torch.__version__}\")\n    print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n    print(\"=\" * 60)\n    print(\"\\nðŸŽ‰ You can now continue with the training cells!\")\n    \nexcept ImportError as e:\n    print(\"=\" * 60)\n    print(\"âŒ IMPORT ERROR!\")\n    print(\"=\" * 60)\n    print(f\"Error: {e}\")\n    print(\"\\nâš ï¸  Did you restart the runtime?\")\n    print(\"   If not, go back and run the restart cell or\")\n    print(\"   manually restart: Runtime â†’ Restart runtime\")\n    print(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47dd5189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d077f12",
   "metadata": {
    "id": "5d077f12"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcdf1f",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a3a9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "Memory: 42.47 GB\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a015be60a34bf39091889e03fecf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af650d7dc7242138c0aafe7d4e199fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1631 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65701cb422774c17b609b4a3d1f1c87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ROUGE from evaluate library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3439530870.py:194: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting training with FLAN-T5 (philschmid/flan-t5-base-samsum config)...\n",
      "Model: google/flan-t5-base\n",
      "Batch size: 8\n",
      "Learning rate: 5e-05\n",
      "Epochs: 5\n",
      "Total training samples: 6522\n",
      "Total validation samples: 1631\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4080' max='4080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4080/4080 55:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.573100</td>\n",
       "      <td>1.377279</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.249584</td>\n",
       "      <td>0.405677</td>\n",
       "      <td>0.406168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.528300</td>\n",
       "      <td>1.360534</td>\n",
       "      <td>0.498709</td>\n",
       "      <td>0.251658</td>\n",
       "      <td>0.407285</td>\n",
       "      <td>0.407561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.492400</td>\n",
       "      <td>1.346205</td>\n",
       "      <td>0.503938</td>\n",
       "      <td>0.251486</td>\n",
       "      <td>0.410301</td>\n",
       "      <td>0.410493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.399200</td>\n",
       "      <td>1.350046</td>\n",
       "      <td>0.507115</td>\n",
       "      <td>0.251601</td>\n",
       "      <td>0.409631</td>\n",
       "      <td>0.409763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.337700</td>\n",
       "      <td>1.352412</td>\n",
       "      <td>0.509212</td>\n",
       "      <td>0.256246</td>\n",
       "      <td>0.410548</td>\n",
       "      <td>0.410590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.334000</td>\n",
       "      <td>1.349421</td>\n",
       "      <td>0.507443</td>\n",
       "      <td>0.255063</td>\n",
       "      <td>0.412513</td>\n",
       "      <td>0.412179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.309500</td>\n",
       "      <td>1.349449</td>\n",
       "      <td>0.503623</td>\n",
       "      <td>0.251690</td>\n",
       "      <td>0.406572</td>\n",
       "      <td>0.406722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.340800</td>\n",
       "      <td>1.347870</td>\n",
       "      <td>0.507316</td>\n",
       "      <td>0.253907</td>\n",
       "      <td>0.409627</td>\n",
       "      <td>0.409892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training completed!\n",
      "================================================================================\n",
      "\n",
      "Saving best model...\n",
      "Model saved to ./flan-t5-best-model\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Model + Tokenizer (FLAN-T5 - Better for Summarization!)\n",
    "# -----------------------------------------\n",
    "model_name = \"google/flan-t5-base\"  # FLAN-T5 is better than regular T5 for summarization\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU\n",
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 80\n",
    "\n",
    "# -----------------------------------------\n",
    "# Preprocess Function\n",
    "# -----------------------------------------\n",
    "def preprocess(batch):\n",
    "    inputs = [\"summarize: \" + d for d in batch[\"dialogue\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"summary\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # IMPORTANT: ignore padding tokens in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in seq]\n",
    "        for seq in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Process with multiple workers for faster data loading\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    num_proc=4\n",
    ")\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Metric (FIXED - Handle decoding properly)\n",
    "# -----------------------------------------\n",
    "try:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    print(\"Successfully loaded ROUGE from evaluate library\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading rouge from evaluate: {e}\")\n",
    "    print(\"Using rouge_score directly instead...\")\n",
    "    from rouge_score import rouge_scorer\n",
    "    rouge = None\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    \n",
    "    # Replace -100 in predictions (shouldn't happen but just in case)\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels with pad_token_id before decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Strip whitespace\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    if rouge is not None:\n",
    "        try:\n",
    "            result = rouge.compute(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "                use_stemmer=True\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error using evaluate rouge: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # Fallback to rouge_score\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "        score = scorer.score(ref, pred)\n",
    "        rouge1_scores.append(score['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(score['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': sum(rouge1_scores) / len(rouge1_scores) if rouge1_scores else 0,\n",
    "        'rouge2': sum(rouge2_scores) / len(rouge2_scores) if rouge2_scores else 0,\n",
    "        'rougeL': sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0,\n",
    "    }\n",
    "\n",
    "# -----------------------------------------\n",
    "# Data Collator\n",
    "# -----------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Training Arguments (Matching philschmid/flan-t5-base-samsum)\n",
    "# -----------------------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-base-dialogue-sum\",\n",
    "    \n",
    "    # Hyperparameters from philschmid/flan-t5-base-samsum\n",
    "    learning_rate=5e-05,                 # Exact match\n",
    "    per_device_train_batch_size=8,       # Exact match\n",
    "    per_device_eval_batch_size=8,        # Exact match\n",
    "    num_train_epochs=5,                  # Exact match (was 3)\n",
    "    \n",
    "    # Seed for reproducibility\n",
    "    seed=42,\n",
    "    \n",
    "    # Optimizer - Using AdamW (closest to Adam) with matching betas\n",
    "    optim=\"adamw_torch\",                 # Adam-like with betas=(0.9,0.999), eps=1e-08\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler_type=\"linear\",          # Exact match\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # A100-specific optimizations\n",
    "    bf16=True,                           # A100 has better bfloat16 support\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=False,        # A100 has enough memory\n",
    "    \n",
    "    # Generation settings\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=80,\n",
    "    generation_num_beams=4,              # Better quality predictions\n",
    "    \n",
    "    # Performance\n",
    "    report_to=\"none\",\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Additional optimizations\n",
    "    group_by_length=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Trainer\n",
    "# -----------------------------------------\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# TRAINING\n",
    "# -----------------------------------------\n",
    "print(\"=\"*80)\n",
    "print(\"Starting training with FLAN-T5 (philschmid/flan-t5-base-samsum config)...\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Total training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Total validation samples: {len(tokenized_dataset['validation'])}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Save the best model\n",
    "# -----------------------------------------\n",
    "print(\"\\nSaving best model...\")\n",
    "trainer.save_model(\"./flan-t5-best-model\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-best-model\")\n",
    "print(\"Model saved to ./flan-t5-best-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b46731ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting inference on test set...\n",
      "================================================================================\n",
      "Model loaded from ./flan-t5-best-model\n",
      "Device: cuda\n",
      "\n",
      "Total test samples: 2210\n",
      "\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [03:43<00:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 2210 predictions\n",
      "\n",
      "================================================================================\n",
      "Submission file created successfully!\n",
      "================================================================================\n",
      "\n",
      "Submission shape: (2210, 2)\n",
      "Saved to: submission.csv\n",
      "\n",
      "================================================================================\n",
      "Sample predictions:\n",
      "================================================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "ID: 13816165\n",
      "Dialogue: Reuben: hey, what are you doing?\n",
      "Lucy: nothing special\n",
      "Lucy: why are you asking?\n",
      "Reuben: I want to take Daisy and Norma for a walk\n",
      "Reuben: I thought you could join us with Marvin\n",
      "Lucy: great idea, mee...\n",
      "Predicted Summary: A will take Daisy and Norma for a walk with Marvin.\n",
      "\n",
      "\n",
      "--- Sample 2 ---\n",
      "ID: 13731502\n",
      "Dialogue: John: Hello, I wanted to ask how should I receive my prize.\n",
      "Walter: In which competition? We have several of them.\n",
      "John: 23rd of November, the prize was a phone.\n",
      "Walter: Okay, let me check and contact...\n",
      "Predicted Summary: B will receive his prize on 23rd of November. The address of the delivery will be in Allentown. B will choose a color for the prize.\n",
      "\n",
      "\n",
      "--- Sample 3 ---\n",
      "ID: 13821053\n",
      "Dialogue: Amanda: Hi! Do you know actually the meaning of your names?\n",
      "Jeff: I think everybody knows, right?\n",
      "Amanda: i've just discovered what my name means, I've never thought about it before\n",
      "Jeff: and?\n",
      "Amanda:...\n",
      "Predicted Summary: Jeff's name is diminutive from Jefferson which means \"peaceful pledge\" and C's is \"crown\".\n",
      "\n",
      "\n",
      "--- Sample 4 ---\n",
      "ID: 13863137\n",
      "Dialogue: John: Mary, where r u?\n",
      "Mary: I'm home now\n",
      "John: What took you so long?\n",
      "Mary: Shopping :)...\n",
      "Predicted Summary: B spent a lot of time shopping.\n",
      "\n",
      "\n",
      "--- Sample 5 ---\n",
      "ID: 13864413\n",
      "Dialogue: James: <file_video>\n",
      "James: Victor's first steps <3\n",
      "Nathalie: He's sooo big!!!\n",
      "Adlene: omg! Well done little boy\n",
      "James: I can't stop him now :D\n",
      "Nathalie: How old is he?\n",
      "James: 10 months in a week\n",
      "Adlen...\n",
      "Predicted Summary: Victor has taken his first steps. He's 10 months old in a week.\n",
      "\n",
      "================================================================================\n",
      "Submission file verification:\n",
      "================================================================================\n",
      "         id                                            summary\n",
      "0  13816165  A will take Daisy and Norma for a walk with Ma...\n",
      "1  13731502  B will receive his prize on 23rd of November. ...\n",
      "2  13821053  Jeff's name is diminutive from Jefferson which...\n",
      "3  13863137                    B spent a lot of time shopping.\n",
      "4  13864413  Victor has taken his first steps. He's 10 mont...\n",
      "5  13680704  B didn't call A when he came back. He had two ...\n",
      "6  13612164  A is working hard at her job. B is going to a ...\n",
      "7  13813262  A is still in bed. B got a guy's number. A doe...\n",
      "8  13818877  A and Cory don't know where the entrance to th...\n",
      "9  13730705  B has been on a meeting all day. She will be a...\n",
      "\n",
      "Submission file is ready for upload!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# INFERENCE ON TEST SET & SUBMISSION GENERATION\n",
    "# -----------------------------------------\n",
    "print(\"=\"*80)\n",
    "print(\"Starting inference on test set...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the best model\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = \"./flan-t5-best-model\"  # Updated to FLAN-T5 path\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Prepare test data\n",
    "test_dialogues = test[\"dialogue\"].fillna(\"\").tolist()\n",
    "test_ids = test[\"id\"].tolist()\n",
    "\n",
    "print(f\"\\nTotal test samples: {len(test_dialogues)}\")\n",
    "\n",
    "# Generate predictions in batches\n",
    "batch_size = 32  # A100 can handle large batches\n",
    "predictions = []\n",
    "\n",
    "print(\"\\nGenerating predictions...\")\n",
    "for i in tqdm(range(0, len(test_dialogues), batch_size)):\n",
    "    batch_dialogues = test_dialogues[i:i+batch_size]\n",
    "    \n",
    "    # Preprocess\n",
    "    inputs = [\"summarize: \" + d for d in batch_dialogues]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **tokenized,\n",
    "            max_length=80,\n",
    "            num_beams=4,              # Use beam search for better quality\n",
    "            length_penalty=0.6,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,   # Avoid repetition\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    batch_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predictions.extend(batch_predictions)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} predictions\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Create submission.csv\n",
    "# -----------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'summary': predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Submission file created successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSubmission shape: {submission_df.shape}\")\n",
    "print(f\"Saved to: submission.csv\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample predictions:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(min(5, len(submission_df))):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"ID: {submission_df.iloc[i]['id']}\")\n",
    "    print(f\"Dialogue: {test_dialogues[i][:200]}...\")\n",
    "    print(f\"Predicted Summary: {submission_df.iloc[i]['summary']}\")\n",
    "    print()\n",
    "\n",
    "# Verify submission format\n",
    "print(\"=\"*80)\n",
    "print(\"Submission file verification:\")\n",
    "print(\"=\"*80)\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nSubmission file is ready for upload!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81566795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " colab_setup.ipynb           'SAMSum_Analysis_Complete_(1).ipynb'\n",
      " \u001b[0m\u001b[01;34mflan-t5-base-dialogue-sum\u001b[0m/   samsum_test_cleaned.csv\n",
      " \u001b[01;34mflan-t5-best-model\u001b[0m/          samsum_train_cleaned.csv\n",
      " KLA_Competition.ipynb        submission.csv\n",
      " \u001b[01;34mpulse-quest-env26\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6233efe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/KLA/flan-t5-best-model\n"
     ]
    }
   ],
   "source": [
    "cd flan-t5-best-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faf45dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09cfed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                            summary\n",
      "0     13816165  A will take Daisy and Norma for a walk with Ma...\n",
      "1     13731502  B will receive his prize on 23rd of November. ...\n",
      "2     13821053  Jeff's name is diminutive from Jefferson which...\n",
      "3     13863137                    B spent a lot of time shopping.\n",
      "4     13864413  Victor has taken his first steps. He's 10 mont...\n",
      "...        ...                                                ...\n",
      "2205  13829361  A and B are going to see The Man that Jack Bui...\n",
      "2206  13864780  Tom is looking for a new employee for the team...\n",
      "2207  13728480  A has just got her nails done, hair dyed lilac...\n",
      "2208  13716555  A, Bradshaw and Cutler will meet for coffee in...\n",
      "2209  13716284  A, Fred and Becky will have lunch at 1 pm in t...\n",
      "\n",
      "[2210 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3828e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# INFERENCE ON TEST SET & SUBMISSION GENERATION\n",
    "# -----------------------------------------\n",
    "print(\"=\"*80)\n",
    "print(\"Starting inference on test set...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the best model\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = \"./flan-t5-best-model\"  # Updated to FLAN-T5 path\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Prepare test data\n",
    "test_dialogues = test[\"dialogue\"].fillna(\"\").tolist()\n",
    "test_ids = test[\"id\"].tolist()\n",
    "\n",
    "print(f\"\\nTotal test samples: {len(test_dialogues)}\")\n",
    "\n",
    "# Generate predictions in batches\n",
    "batch_size = 32  # A100 can handle large batches\n",
    "predictions = []\n",
    "\n",
    "print(\"\\nGenerating predictions...\")\n",
    "for i in tqdm(range(0, len(test_dialogues), batch_size)):\n",
    "    batch_dialogues = test_dialogues[i:i+batch_size]\n",
    "    \n",
    "    # Preprocess\n",
    "    inputs = [\"summarize: \" + d for d in batch_dialogues]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **tokenized,\n",
    "            max_length=80,\n",
    "            num_beams=4,              # Use beam search for better quality\n",
    "            length_penalty=0.6,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,   # Avoid repetition\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    batch_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predictions.extend(batch_predictions)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} predictions\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Create submission.csv\n",
    "# -----------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'summary': predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Submission file created successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSubmission shape: {submission_df.shape}\")\n",
    "print(f\"Saved to: submission.csv\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample predictions:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(min(5, len(submission_df))):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"ID: {submission_df.iloc[i]['id']}\")\n",
    "    print(f\"Dialogue: {test_dialogues[i][:200]}...\")\n",
    "    print(f\"Predicted Summary: {submission_df.iloc[i]['summary']}\")\n",
    "    print()\n",
    "\n",
    "# Verify submission format\n",
    "print(\"=\"*80)\n",
    "print(\"Submission file verification:\")\n",
    "print(\"=\"*80)\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nSubmission file is ready for upload!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03f50334df724707bc6b5773c97b2837": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09cf85e5da7f474bb064939b6bd1dec8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25259d477a944942b9fd564245fda1ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a67b8cd9b6b424ead1032cea4dca377",
      "max": 8249,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_636d196076a24735b04ec01e026e4f7c",
      "value": 8249
     }
    },
    "2b7fa6bb6c044e0295dedde2d8ae6c5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec5dc8f84e554c9cbe35c9406fdbe613",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c15914cea21b43718f775b8a563d36a3",
      "value": "â€‡2063/2063â€‡[00:00&lt;00:00,â€‡3448.79â€‡examples/s]"
     }
    },
    "38edcf9de1d641dab3ec09f3a5955f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b53527b6d1241cf92166f8b40cc5b3a",
       "IPY_MODEL_25259d477a944942b9fd564245fda1ba",
       "IPY_MODEL_4d0f874ce9bd4e16b5ca671c80be455e"
      ],
      "layout": "IPY_MODEL_519b7682020d40e9889a3d9f24e3ba3e"
     }
    },
    "3a67b8cd9b6b424ead1032cea4dca377": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475766ee611e4aba9d20179ce0dea64c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fefb981e19424b02abbc6987f66c7334",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_66b30a29629544c2b85fb0ee3c911439",
      "value": "Map:â€‡100%"
     }
    },
    "4d0f874ce9bd4e16b5ca671c80be455e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3f63ed9f4564a5590b55f8e9928349d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_87ff0d98071d4c4f8ef48025dde5170b",
      "value": "â€‡8249/8249â€‡[00:02&lt;00:00,â€‡2858.52â€‡examples/s]"
     }
    },
    "519b7682020d40e9889a3d9f24e3ba3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5789d37d939143b8a9ec8d6466c8335c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8d90a7457144d20aabdb30488a45dcc",
      "max": 2063,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9967456b37d4f0a91e69a22e2f466b3",
      "value": 2063
     }
    },
    "636d196076a24735b04ec01e026e4f7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66b30a29629544c2b85fb0ee3c911439": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b53527b6d1241cf92166f8b40cc5b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09cf85e5da7f474bb064939b6bd1dec8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_84c4644c8e3e41ac8abd1a4d06436844",
      "value": "Map:â€‡100%"
     }
    },
    "84c4644c8e3e41ac8abd1a4d06436844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87ff0d98071d4c4f8ef48025dde5170b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3f63ed9f4564a5590b55f8e9928349d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c15914cea21b43718f775b8a563d36a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9967456b37d4f0a91e69a22e2f466b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec5dc8f84e554c9cbe35c9406fdbe613": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8d90a7457144d20aabdb30488a45dcc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcd0c5ebb6b54d77a4e1fc1c21aec25b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_475766ee611e4aba9d20179ce0dea64c",
       "IPY_MODEL_5789d37d939143b8a9ec8d6466c8335c",
       "IPY_MODEL_2b7fa6bb6c044e0295dedde2d8ae6c5f"
      ],
      "layout": "IPY_MODEL_03f50334df724707bc6b5773c97b2837"
     }
    },
    "fefb981e19424b02abbc6987f66c7334": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}